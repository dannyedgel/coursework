%%% Econ717: Applied Econometrics
%%% Spring 2022
%%% Danny Edgel
%%%
% Due on Canvas Wednesday, March 9th, 11:00 AM
%%%

%%%
%							PREAMBLE
%%%

\documentclass{article}

%%% declare packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{bbm}
\usepackage{changepage}
\usepackage{centernot}
\usepackage{color}
\usepackage{courier}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[shortlabels]{enumitem}
\usepackage{boondox-cal}
\usepackage{fancyhdr}
	\fancyhf{} % sets both header and footer to nothing
	\renewcommand{\headrulewidth}{0pt}
    \rfoot{Edgel, \thepage}
    \pagestyle{fancy}
	
%%% define shortcuts for set notation
\newcommand{\N}{\mathcal{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\union}{\bigcup}
\newcommand{\intersect}{\bigcap}
\newcommand{\lmt}{\underset{x\rightarrow\infty}{\text{lim }}}
\newcommand{\neglmt}{\underset{n\rightarrow-\infty}{\text{lim }}}
\newcommand{\zerolmt}{\underset{x\rightarrow 0}{\text{lim }}}
\newcommand{\usmax}{\underset{1\leq k \leq n}{\text{max }}}
\newcommand{\usmin}[1]{\underset{#1}{\text{min }}}
\newcommand{\intinf}{\int_{-\infty}^{\infty}}
\newcommand{\olx}[1]{\overline{X}_{#1}}
\newcommand{\oly}[1]{\overline{Y}_{#1}}
\newcommand{\olz}[1]{\overline{Z}_{#1}}
%\newcommand{\est}[1]{\frac{1}{#1}\sum_{i=1}^{#1}}
\newcommand{\est}[1]{\frac{1}{\lowercase{#1}}\sum_{i=1}^{\lowercase{#1}}}
\newcommand{\sumn}{\sum_{i=1}^{n}}
\newcommand{\loge}[1]{\text{log}\left(#1\right)}
\renewcommand{\tilde}[1]{\widetilde{#1}}
\newcommand{\tb}{\tilde{\beta}}
\renewcommand{\Pr}[1]{\text{Pr}\left(#1\right)}
\newcommand{\bols}{\hat{\beta}^{OLS}}
\newcommand{\bhat}{\hat{\beta}}
\newcommand{\ahat}{\hat{\alpha}}
\newcommand{\ehat}{\hat{\varepsilon}}
\newcommand{\vols}{\hat{\varepsilon}_{OLS}}
\newcommand{\one}[1]{\mathbbm{1}\left\{#1\right\}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\pfrac}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\bcls}{\tilde{\beta}_{CLS}}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\vt}{\tilde{\varepsilon}}
\renewcommand{\Pr}[1]{Pr\left(#1\right)}
\newcommand{\biv}{\bhat^{IV}}
\newcommand{\xbar}{\overline{X}}
\newcommand{\ybar}{\overline{Y}}
\newcommand{\zbar}{\overline{Z}}
\newcommand{\eps}{\varepsilon}
\newcommand{\esti}{\frac{1}{T_i-1}\sum_{t=1}^{T_i}}
\newcommand{\oinv}{\Omega^{-1}}
\newcommand{\olg}{\overline{g}_n}
\newcommand{\e}[1]{\text{exp}\left(#1\right)}
\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}
\newcommand{\that}{\hat{\theta}_n}
\newcommand{\tshat}{\hat{\theta}^*_n}
\newcommand{\ttilde}{\tilde{\theta}_n}
\newcommand{\ghat}{\hat{\gamma}_n}
\newcommand{\gtilde}{\tilde{\gamma}_n}
\newcommand{\chat}{\hat{c}}
\newcommand{\Qhat}{\hat{Q}_n(\beta)}
\renewcommand{\lim}[1]{\underset{#1}{\text{lim }}}
\newcommand{\xs}{X^*}
\newcommand{\olxs}{\overline{X}^*}
\newcommand{\pinv}{\Phi^{-1}}
\newcommand{\tchat}{\that^\dagger}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}% expected value
\newcommand{\Es}[1]{\mathbb{E}^*\left[#1\right]}% expected value
\renewcommand{\exp}[1]{\E\left[#1\right]}

\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}


%%% define column vector command (from Michael Nattinger)
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}
\newcount\rowveccount
\newcommand*\rowvec[1]{
        \global\rowveccount#1
        \begin{pmatrix}
        \rowvecnext
}
\def\rowvecnext#1{
        #1
        \global\advance\rowveccount-1
        \ifnum\rowveccount>0
                &
                \expandafter\rowvecnext
        \else
                \end{pmatrix}
        \fi
}

\makeatletter
\let\amsmath@bigm\bigm

\renewcommand{\bigm}[1]{%
  \ifcsname fenced@\string#1\endcsname
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\expandafter\amsmath@bigm\csname fenced@\string#1\endcsname}%
  {\amsmath@bigm#1}%
}


%________________________________________________________________%

\begin{document}

\title{	Problem Set \#2 }
\author{ 	Danny Edgel 			\\ 
		Econ 717: Applied Econometrics	\\
		Spring 2022						
		}
\maketitle\thispagestyle{empty}

%%%________________________________________________________________%%%

\noindent The attached file, edgel\_ps2\_log.log, includes all console output for this problem set. Table 1, below, displays the results of the regression that determines the average treatment on the treated (ATT) using the exerimental group. Despite the fact that the treatment is, by design, uncorrelated with all coviates, we may still want to include the covariates in the regression in order to account for the selection of would-be treatment-receivers across the covariates, since the dependent variable still depends on the covariates. Excluding these covariates would result in a consistent but inefficient estimate of the ATT.
\pagebreak
\begin{center}
        \textbf{Table 1: Experimental impact regresison results} \\
        \input{table1.tex}
\end{center} 

\noindent Table 2 displays the distributional statistics of the two propensity score estimates, and Figure 1 displays their histograms. The ``completely determined'' observations in the probit estimation are the result of one or more observations having no variation in a given set of covariate and outcome values. For example, if the regression were run on three dummy variables, and the third variable only equaled 1 for observations that were also equal in the other two dummies and were also treated, then the model would recognize them as ``completely determined'' at $p=1$.
\begin{center}
        \textbf{Table 2: Summary statistics of propensity scores by sample group} \\
        \input{table2.tex}
\end{center}
\noindent As you can see, the untreated propensity scores for the untreated observations are far more bunched near zero than the treated observations, over three quarters of which are above 0.10 for both the rich and coarse scores. Meanwhile, over 95\% of the untreated observations have both rich and coarse propensity scores below 0.07. This stark difference in distributions is noticeable, as well, in the histogram of each propensity score, which are displayed in Figure 1.
\begin{center}
        \textbf{Figure 1: Histogram of propensity scores} \\
        \includegraphics[scale=.75]{q5.png}
\end{center}
\noindent What this suggests is that a propensity score analysis would not be appropriate for this sample. Any such analysis will rely heavily on the small number of treated observations with extremely low propensity scores, and any treatment effect is likely to be statistically insignificant, due to most of the treated sample going unused for lack of a comparable control.

The non-experimental bias estimates using both estimated propensity scores and nearest-neighbor matching without replacement are not reported in any tables, but can be seen in the log file. \input{q6a.tex}\input{q6b.tex}\input{q7a.tex}\input{q7b.tex} Allowing comparison group observations to be reused substantially reduces bias in this context. This is due to the reality observed in the distribution statistics and plot, that there are only a handful of control observations with propensity scores above 0.10, where most of the treatment observations are. Thus, far fewer treatment observations are required to match with observations far from them on the propensity score axis in order to match when replacement is used.

\input{q8.tex}All remaining non-experimental bias estimates calculcated for this problem set are displayed in Table 3. The nearest-neighbor result with replacement for the rich propensity score estimate is included at the top for comparison.
\pagebreak
\begin{center}
        \textbf{Table 3: Average treatment effect on the treated and estimated bias by estimation method, using the rich propensity score estimate} \\
        \input{table3.tex}
\end{center}
\noindent Using a Gaussian kernel with \textit{any} bandwidth results in the same ATT estimate (to the first decimal place) as the nearest-neighbor result with replacement. This is also likely due to the fact that the common support condition results in some 90\% of the treatment observations all being matched to a single control observation regardless of bandwitch, even when using a Gaussian kernel.

Using local linear matching, however, increasing the bandwidth meaningfully reduces the bias, ultimately changing the sign of the ATT at a bandwidth of 2.0, making it consistent with the true ATT, which does not occur with any other estimator we consider.

Estimating a simple linear regression on the control variables for the rich propensity score results in an ATT of -1936.9, which is less biased than the local linear estimate with a bandwidth at or below 0.2, but more biased than the nearest-neighbor estimate with replacement or the Gaussian kernel matching estimator. Calculating the ATT using the difference between the actual and predicted income in 1978 (based on a regression that uses only untreated observations on the rich score covariates) results in a slightly more biased ATT of -1941.8. These estimates differ due to correlation between the treatment selection and the covariates. However, the difference between the two is not statistically significant, since the standard error of the coefficient on the treatment variable is 341.3.

The ATT estimate using unscaled inverse propensity weighting (IPW) is -1887.1, which is more biased than the nearest-neighbor and Guassian kernel matching estimators, but the scaled IPW estimate is -1640.8, which is less biased than any other estimator from this problem set other than the local linear estimate with a bandwidth of 2.0. This is likely due to the large scale of the dependent variable, which should have been scaled or estimated in logs in the first place.
\end{document}