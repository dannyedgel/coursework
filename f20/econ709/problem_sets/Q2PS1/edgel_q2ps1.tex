%%% Econ709: Econometrics
%%% Fall 2020
%%% Danny Edgel
%%%
% Due on Canvas Wednesday, November 11th, 11:59pm Central Time
%%%

%%%
%							PREAMBLE
%%%

\documentclass{article}

%%% declare packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{bbm}
\usepackage{changepage}
\usepackage{centernot}
\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
	\fancyhf{} % sets both header and footer to nothing
	\renewcommand{\headrulewidth}{0pt}
    \rfoot{Edgel, \thepage}
    \pagestyle{fancy}
	
%%% define shortcuts for set notation
\newcommand{\N}{\mathcal{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\union}{\bigcup}
\newcommand{\intersect}{\bigcap}
\newcommand{\lmt}{\underset{x\rightarrow\infty}{\text{lim }}}
\newcommand{\neglmt}{\underset{n\rightarrow-\infty}{\text{lim }}}
\newcommand{\zerolmt}{\underset{x\rightarrow 0}{\text{lim }}}
\newcommand{\usmax}{\underset{1\leq k \leq n}{\text{max }}}
\newcommand{\intinf}{\int_{-\infty}^{\infty}}
\newcommand{\olx}[1]{\overline{X}_{#1}}
\newcommand{\oly}[1]{\overline{Y}_{#1}}
\newcommand{\est}[1]{\frac{1}{#1}\sum_{i=1}^{#1}}
\newcommand{\sumn}{\sum_{i=1}^{n}}
\newcommand{\loge}[1]{\text{log}\left(#1\right)}
\newcommand{\tb}{\tilde{\beta}}
\renewcommand{\Pr}[1]{\text{Pr}\left(#1\right)}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\E}{\mathbb{E}}% expected value

%%% define column vector command (from Michael Nattinger)
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\makeatletter
\let\amsmath@bigm\bigm

\renewcommand{\bigm}[1]{%
  \ifcsname fenced@\string#1\endcsname
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\expandafter\amsmath@bigm\csname fenced@\string#1\endcsname}%
  {\amsmath@bigm#1}%
}


%________________________________________________________________%

\begin{document}

\title{	Problem Set \#6 }
\author{ 	Danny Edgel 										\\ 
			Econ 709: Economic Statistics and Econometrics I	\\
			Fall 2020											\\
		}
\maketitle\thispagestyle{empty}

%%%________________________________________________________________%%%

\noindent\textit{Collaborated with Sarah Bass, Emily Case, Michael Nattinger, and Alex Von Hafften}
%%%________________________________________________________________%%%

\section*{Question 1}
\textbf{Find} $\mathbf{\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1]}$
\bigskip \\
By the Law of Iterated Expectation,
\begin{align*}
	\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1] &= \E[\E[Y|X_1,X_2]|X_1]	\\
	\E[\E[Y|X_1,X_2]|X_1] &= \E[Y|X_1]
\end{align*}
Thus, $\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1]=\E[Y|X_1]$


%%%________________________________________________________________%%%

\section*{Question 2}
\textbf{Prove that for any function $h(x)$ such that $\E|h(X)e|<\infty$ then ${\E[h(X)e]=0}$, where $e=Y-m(X)$ and $m(X)=\E[Y|X]$} 
\bigskip \\
According to the conditioning theorem, if $\E|Y|<\infty$, then
\[
	\E[g(X)Y|X] = g(X)\E[Y|X]
\]
Thus, Since $\E|h(X)e|<\infty$ trivially implies $\E|Y|<\infty$, we can use the Law of Iterated Expectation to solve:
\begin{align*}
	\E[h(X)e] 	&= \E[h(X)Y-h(X)m(X)] = \E[h(X)Y] - \E[h(X)m(X)]	\\
				&= \E[\E[h(X)Y|X]] - \E[h(X)m(X)] = \E[h(X)\E[Y|X]] - \E[h(X)m(X)]	\\
				&= \E[h(X)m(X)] - \E[h(X)m(X)] = 0
\end{align*}
$\therefore$ for any function $h(x)$ such that $\E|h(X)e|<\infty$ then ${\E[h(X)e]=0}$ $\blacksquare$


%%%________________________________________________________________%%%
\pagebreak
\section*{Question 3}
\begin{align*}
	\E[Y|X] &= \begin{cases} .4, & X= 0 \\ .3, & X = 1 \end{cases} \\
	\E[Y^2|X] &= \begin{cases} .4, & X= 0 \\ .3, & X = 1 \end{cases} \\
	Var(Y|X) &= \E[Y^2|X] - \left(\E[Y|X]\right)^2 = \begin{cases} .24, & X= 0 \\ .21, & X = 1 \end{cases} 
\end{align*}

%%%________________________________________________________________%%%

\section*{Question 4}
\textbf{Show that $\sigma^2(X)$ minimizes the mean-squared error and is thus the best predictor.}
\bigskip \\
The variance of $\hat{\beta}_{OLS}=\E(Y|X)$ is
\[
	\sigma^2(X) = \E\left[\left(Y-h(X)\right)^2\right] = \sigma^2(X'X)^{-1}
\]
Where $\sigma^2=\E(\varepsilon|X)$ and $h(X)$ is the predictor of $Y$, which, in this case, is $E(Y|X)$. It is clear that minimizing the variance of $\hat{\beta}$ will also minimize mean-squared error. Thus, we can show that this minimizes mean-squared error among all linear unbiased estimators by comparing this variance to the variance of an arbitrary linear  estimator, $\tilde{\beta}=a+Ay$. In order for $\tb$ to be unbiased, it must be the case that ${\E(\tb)=\E(\tb|X)=\beta}$. Thus,
\begin{align*}
	\beta &= \E(\tb|X) = \E(a+Ay|X) = a + A\E(y|x) = a + A\beta
\end{align*}
This only holds if $a=0$, so $\tb=Ay$. Then, the variance of $\tb$ is:
\begin{align*}
	\E(\tb|X) 	&= V(Ay|X) = AV(y|X)A' = \sigma^2AA'														\\
				&= \sigma^2\left[A - (X'X)^{-1}+(X'X)^{-1}X'\right]\left[A'- (X'X)^{-1}+(X'X)^{-1}X'\right]	\\
				&= \text{... skipping intermediate steps for brevity}										\\
				&= \sigma^2[A-(X'X)^{-1}X'][A-(X'X)^{-1}X']' + \sigma^2(X'X)^{-1}X'X(X'X)^{-1}				\\
				&= \sigma^2(X) + \sigma^2[A-(X'X)^{-1}X'][A-(X'X)^{-1}X']'									\\
				&\geq \sigma^2(X)
\end{align*}
Thus, $\sigma^2(X)$ minimizes the mean-squared error over any other linear estimator.

%%%________________________________________________________________%%%
\pagebreak
\section*{Question 5}
\textbf{Compute $\E[Y|X]$ and $Var(Y|X)$ for a Poisson-distributed $Y$ given $X$. Does this justify a linear regression model?}
\bigskip \\
The conditional mean and variance are:
\[
	\E[Y|X]=x'\beta\text{ ,  }Var(Y|X)=x'\beta
\]
A linear regression model is satisfied if the three Gauss-Markov assumptions are violated. Provided $\text{rank}X=k$, that is the case in this situation, because $Var(Y|X)=x'$ implies that the model has constant variance, and:
\[
	\E[Y|X]=x'\beta \iff \E(\varepsilon|X) = 0
\]
Thus, a linear regression model is justified.

%%%________________________________________________________________%%%

\section*{Question 6}
\begin{itemize}
	\item[2.10] True. By the law of iterated expectations,
		\[
			\E[X^2e] = \E[\E[X^2e|X]] = \E[X^2\E[e|X]] = 0
		\]
	\item[2.11] False. Consider $X=\{-1,1\}$ with constant error ${e=\overline{e}}$ and ${\text{Pr}(X=-1)=\text{Pr}(X=1)=\frac{1}{2}}$. In this case, ${\E[Xe]=0}$ and ${\E[X^2e]=\overline{e}}$
	\item[2.12] True. $\E[e]=0$ by definition, so $e$ is independent of $X$ if and only if ${\E[Xe]=\E[e]\E[X]=0}$. By the law of iterated expectations,
		\[
			\E[Xe] = \E[\E[Xe|X]] = \E[X\E[e|X]] = 0
		\]
	\item[2.13] False. Consider the same example from 2.11. $E[eX]=0$, but $\E[e|X]=\overline{e}$.
	\item[2.14] True. Using the law of iterated examples explanation from 2.12, ${\E[e|X]=0\Rightarrow\E[Xe]=0}$.
\end{itemize}


%%%________________________________________________________________%%%
\pagebreak
\section*{Question 7}
\textbf{Let $X$ and $Y$ have the joint density ${f(x,y)=\frac{3}{2}(x^2+y^2)}$ on ${0\leq x\leq 1,\text{ }0\leq y\leq 1}$. Compute the coefficients of the best linear predictor ${Y=\alpha+\beta X+e}$. Compute the conditional expectation ${m(x)=\E[Y|X=x]}$. Are the best linear predictor and conditional expectation different?}
\bigskip \\
The conditional expectation, ${m(x)=\E[Y|X=x]}$, can be calculated as:
\begin{align*}
	f(y|x)		&= \frac{f(x,y)}{\int^1_0f(x,y)dy} = \frac{\frac{3}{2}(x^2+y^2)}{\int^1_0\frac{3}{2}(x^2+y^2)dy} = \frac{x^2+y^2}{x^2+\frac{1}{3}} 			\\
	\E[Y|X=x] 	&= \int_0^1 f(y|x)ydy = \int_0^1 \frac{x^2+y^2}{x^2+\frac{1}{3}}ydy = \frac{1}{x^2+\frac{1}{3}}\left(x^2\int^1_0 ydy+\int^1_0 y^3dy	\right)	\\
				&= \frac{1}{x^2+\frac{1}{3}}\left(\frac{1}{2}x^2+\frac{1}{4}\right) = \frac{2x^2+1}{4x^2+4/3}	\\
				&= \frac{6x^2+3}{12x^2+4}
\end{align*}
And the coefficients $\alpha$ and $\beta$ can be calculated as:
\[
	\colvec{2}{\alpha}{\beta} = \colvec{2}{(\E(Y)-\E(X)\beta}{\frac{\E(XY) - \E(X)\E(Y)}{\E(X^2)-(\E(X))^2}} = \frac{1}{\E(X^2)-(\E(X))^2}\colvec{2}{\E(Y)\E(X^2) - \E(X)\E(XY)}{\E(XY) - \E(X)\E(Y)}
\]
Where:
\begin{align*}
	f(x)	&= \int_0^1 f(x,y)dy = \int_0^1 \frac{3}{2}(x^2+y^2)dy = \frac{3}{2}x^2 + \int_0^1\frac{3}{2}y^2dy = \frac{3}{2}x^2 + \frac{1}{2}	\\
	\E(X) 	&= \int_0^1 f(x)xdx = \int_0^1 \frac{3}{2}x^3dx + \frac{1}{2}\int_0^1xdx = \frac{5}{8}\\
	\E(X^2)	&= \int_0^1 f(x)x^2dx = \int_0^1 \frac{3}{2}x^4dx + \frac{1}{2}\int_0^1x^2dx = \frac{7}{15}	\\
	\E(XY) 	&= \int_0^1 \int_0^1 f(x,y)xydxdy = \int_0^1 \int_0^1 \frac{3}{2}(x^2+y^2)xydxdy \\
			&= \text{... (intermediate steps omitted for brevity) } \\
	\E(XY) 	&= \frac{3}{8} 
\end{align*}
Since $X$ and $Y$ have symmetric marginal distributions, $\E(X)=\E(Y)$. Thus,
\[
	\colvec{2}{\alpha}{\beta} = \frac{1}{\frac{7}{15} - \frac{25}{64}}\colvec{2}{\left(\frac{5}{8}\right)\left(\frac{7}{15}\right) - \left(\frac{5}{8}\right)\left(\frac{3}{8}\right)}{\left(\frac{3}{8}\right) - \left(\frac{5}{8}\right)\left(\frac{5}{8}\right)} = \frac{1}{73}\colvec{2}{55}{15}
\]

%%%________________________________________________________________%%%
\pagebreak
\section*{Question 8}
4.1 - 4.6



%%%________________________________________________________________%%%




\end{document}












