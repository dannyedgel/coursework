%%% Econ709: Econometrics
%%% Fall 2020
%%% Danny Edgel
%%%
% Due on Canvas Wednesday, November 11th, 11:59pm Central Time
%%%

%%%
%							PREAMBLE
%%%

\documentclass{article}

%%% declare packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{bbm}
\usepackage{changepage}
\usepackage{centernot}
\usepackage{graphicx}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr}
	\fancyhf{} % sets both header and footer to nothing
	\renewcommand{\headrulewidth}{0pt}
    \rfoot{Edgel, \thepage}
    \pagestyle{fancy}
	
%%% define shortcuts for set notation
\newcommand{\N}{\mathcal{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\union}{\bigcup}
\newcommand{\intersect}{\bigcap}
\newcommand{\lmt}{\underset{x\rightarrow\infty}{\text{lim }}}
\newcommand{\neglmt}{\underset{n\rightarrow-\infty}{\text{lim }}}
\newcommand{\zerolmt}{\underset{x\rightarrow 0}{\text{lim }}}
\newcommand{\usmax}{\underset{1\leq k \leq n}{\text{max }}}
\newcommand{\intinf}{\int_{-\infty}^{\infty}}
\newcommand{\olx}[1]{\overline{X}_{#1}}
\newcommand{\oly}[1]{\overline{Y}_{#1}}
\newcommand{\est}[1]{\frac{1}{#1}\sum_{i=1}^{#1}}
\newcommand{\sumn}{\sum_{i=1}^{n}}
\newcommand{\loge}[1]{\text{log}\left(#1\right)}
\newcommand{\tb}{\tilde{\beta}}
\renewcommand{\Pr}[1]{\text{Pr}\left(#1\right)}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\E}{\mathbb{E}}% expected value

%%% define column vector command (from Michael Nattinger)
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\makeatletter
\let\amsmath@bigm\bigm

\renewcommand{\bigm}[1]{%
  \ifcsname fenced@\string#1\endcsname
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\expandafter\amsmath@bigm\csname fenced@\string#1\endcsname}%
  {\amsmath@bigm#1}%
}


%________________________________________________________________%

\begin{document}

\title{	Problem Set \#6 }
\author{ 	Danny Edgel 										\\ 
			Econ 709: Economic Statistics and Econometrics I	\\
			Fall 2020											\\
		}
\maketitle\thispagestyle{empty}

%%%________________________________________________________________%%%

\noindent\textit{Collaborated with Sarah Bass, Emily Case, Michael Nattinger, and Alex Von Hafften}
%%%________________________________________________________________%%%

\section*{Question 1}
\textbf{Find} $\mathbf{\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1]}$
\bigskip \\
By the Law of Iterated Expectation,
\begin{align*}
	\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1] &= \E[\E[Y|X_1,X_2]|X_1]	\\
	\E[\E[Y|X_1,X_2]|X_1] &= \E[Y|X_1]
\end{align*}
Thus, $\E[\E[\E[Y|X_1,X_2,X_3]|X_1,X_2]|X_1]=\E[Y|X_1]$


%%%________________________________________________________________%%%

\section*{Question 2}
\textbf{Prove that for any function $h(x)$ such that $\E|h(X)e|<\infty$ then ${\E[h(X)e]=0}$, where $e=Y-m(X)$ and $m(X)=\E[Y|X]$} 
\bigskip \\
According to the conditioning theorem, if $\E|Y|<\infty$, then
\[
	\E[g(X)Y|X] = g(X)\E[Y|X]
\]
Thus, Since $\E|h(X)e|<\infty$ trivially implies $\E|Y|<\infty$, we can use the Law of Iterated Expectation to solve:
\begin{align*}
	\E[h(X)e] 	&= \E[h(X)Y-h(X)m(X)] = \E[h(X)Y] - \E[h(X)m(X)]	\\
				&= \E[\E[h(X)Y|X]] - \E[h(X)m(X)] = \E[h(X)\E[Y|X]] - \E[h(X)m(X)]	\\
				&= \E[h(X)m(X)] - \E[h(X)m(X)] = 0
\end{align*}
$\therefore$ for any function $h(x)$ such that $\E|h(X)e|<\infty$ then ${\E[h(X)e]=0}$ $\blacksquare$


%%%________________________________________________________________%%%
\pagebreak
\section*{Question 3}
\begin{align*}
	\E[Y|X] &= \begin{cases} .4, & X= 0 \\ .3, & X = 1 \end{cases} \\
	\E[Y^2|X] &= \begin{cases} .4, & X= 0 \\ .3, & X = 1 \end{cases} \\
	Var(Y|X) &= \E[Y^2|X] - \left(\E[Y|X]\right)^2 = \begin{cases} .24, & X= 0 \\ .21, & X = 1 \end{cases} 
\end{align*}

%%%________________________________________________________________%%%

\section*{Question 4}
\textbf{Show that $\sigma^2(X)$ minimizes the mean-squared error and is thus the best predictor.}
\bigskip \\
The variance of $\hat{\beta}_{OLS}=\E(Y|X)$ is
\[
	\sigma^2(X) = \E\left[\left(Y-h(X)\right)^2\right] = \sigma^2(X'X)^{-1}
\]
Where $\sigma^2=\E(\varepsilon|X)$ and $h(X)$ is the predictor of $Y$, which, in this case, is $E(Y|X)$. It is clear that minimizing the variance of $\hat{\beta}$ will also minimize mean-squared error. Thus, we can show that this minimizes mean-squared error among all linear unbiased estimators by comparing this variance to the variance of an arbitrary linear  estimator, $\tilde{\beta}=a+Ay$. In order for $\tb$ to be unbiased, it must be the case that ${\E(\tb)=\E(\tb|X)=\beta}$. Thus,
\begin{align*}
	\beta &= \E(\tb|X) = \E(a+Ay|X) = a + A\E(y|x) = a + A\beta
\end{align*}
This only holds if $a=0$, so $\tb=Ay$. Then, the variance of $\tb$ is:
\begin{align*}
	\E(\tb|X) 	&= V(Ay|X) = AV(y|X)A' = \sigma^2AA'														\\
				&= \sigma^2\left[A - (X'X)^{-1}+(X'X)^{-1}X'\right]\left[A'- (X'X)^{-1}+(X'X)^{-1}X'\right]	\\
				&= \text{... skipping intermediate steps for brevity}										\\
				&= \sigma^2[A-(X'X)^{-1}X'][A-(X'X)^{-1}X']' + \sigma^2(X'X)^{-1}X'X(X'X)^{-1}				\\
				&= \sigma^2(X) + \sigma^2[A-(X'X)^{-1}X'][A-(X'X)^{-1}X']'									\\
				&\geq \sigma^2(X)
\end{align*}
Thus, $\sigma^2(X)$ minimizes the mean-squared error over any other linear estimator.

%%%________________________________________________________________%%%
\pagebreak
\section*{Question 5}
\textbf{Compute $\E[Y|X]$ and $Var(Y|X)$ for a Poisson-distributed $Y$ given $X$. Does this justify a linear regression model?}
\bigskip \\
The conditional mean and variance are:
\[
	\E[Y|X]=x'\beta\text{ ,  }Var(Y|X)=x'\beta
\]
A linear regression model is satisfied if the three Gauss-Markov assumptions are violated. Provided $\text{rank}X=k$, that is the case in this situation, because $Var(Y|X)=x'$ implies that the model has constant variance, and:
\[
	\E[Y|X]=x'\beta \iff \E(\varepsilon|X) = 0
\]
Thus, a linear regression model is justified.

%%%________________________________________________________________%%%

\section*{Question 6}
\begin{itemize}
	\item[2.10] True. By the law of iterated expectations,
		\[
			\E[X^2e] = \E[\E[X^2e|X]] = \E[X^2\E[e|X]] = 0
		\]
	\item[2.11] False. Consider $X=\{-1,1\}$ with constant error ${e=\overline{e}}$ and ${\text{Pr}(X=-1)=\text{Pr}(X=1)=\frac{1}{2}}$. In this case, ${\E[Xe]=0}$ and ${\E[X^2e]=\overline{e}}$
	\item[2.12] True. $\E[e]=0$ by definition, so $e$ is independent of $X$ if and only if ${\E[Xe]=\E[e]\E[X]=0}$. By the law of iterated expectations,
		\[
			\E[Xe] = \E[\E[Xe|X]] = \E[X\E[e|X]] = 0
		\]
	\item[2.13] False. Consider the same example from 2.11. $E[eX]=0$, but $\E[e|X]=\overline{e}$.
	\item[2.14] True. Using the law of iterated examples explanation from 2.12, ${\E[e|X]=0\Rightarrow\E[Xe]=0}$.
\end{itemize}


%%%________________________________________________________________%%%
\pagebreak
\section*{Question 7}
\textbf{Let $X$ and $Y$ have the joint density ${f(x,y)=\frac{3}{2}(x^2+y^2)}$ on ${0\leq x\leq 1,\text{ }0\leq y\leq 1}$. Compute the coefficients of the best linear predictor ${Y=\alpha+\beta X+e}$. Compute the conditional expectation ${m(x)=\E[Y|X=x]}$. Are the best linear predictor and conditional expectation different?}
\bigskip \\
The conditional expectation, ${m(x)=\E[Y|X=x]}$, can be calculated as:
\begin{align*}
	f(y|x)		&= \frac{f(x,y)}{\int^1_0f(x,y)dy} = \frac{\frac{3}{2}(x^2+y^2)}{\int^1_0\frac{3}{2}(x^2+y^2)dy} = \frac{x^2+y^2}{x^2+\frac{1}{3}} 			\\
	\E[Y|X=x] 	&= \int_0^1 f(y|x)ydy = \int_0^1 \frac{x^2+y^2}{x^2+\frac{1}{3}}ydy = \frac{1}{x^2+\frac{1}{3}}\left(x^2\int^1_0 ydy+\int^1_0 y^3dy	\right)	\\
				&= \frac{1}{x^2+\frac{1}{3}}\left(\frac{1}{2}x^2+\frac{1}{4}\right) = \frac{2x^2+1}{4x^2+4/3}	\\
				&= \frac{6x^2+3}{12x^2+4}
\end{align*}
And the coefficients $\alpha$ and $\beta$ can be calculated as:
\[
	\colvec{2}{\alpha}{\beta} = \colvec{2}{(\E(Y)-\E(X)\beta}{\frac{\E(XY) - \E(X)\E(Y)}{\E(X^2)-(\E(X))^2}} = \frac{1}{\E(X^2)-(\E(X))^2}\colvec{2}{\E(Y)\E(X^2) - \E(X)\E(XY)}{\E(XY) - \E(X)\E(Y)}
\]
Where:
\begin{align*}
	f(x)	&= \int_0^1 f(x,y)dy = \int_0^1 \frac{3}{2}(x^2+y^2)dy = \frac{3}{2}x^2 + \int_0^1\frac{3}{2}y^2dy = \frac{3}{2}x^2 + \frac{1}{2}	\\
	\E(X) 	&= \int_0^1 f(x)xdx = \int_0^1 \frac{3}{2}x^3dx + \frac{1}{2}\int_0^1xdx = \frac{5}{8}\\
	\E(X^2)	&= \int_0^1 f(x)x^2dx = \int_0^1 \frac{3}{2}x^4dx + \frac{1}{2}\int_0^1x^2dx = \frac{7}{15}	\\
	\E(XY) 	&= \int_0^1 \int_0^1 f(x,y)xydxdy = \int_0^1 \int_0^1 \frac{3}{2}(x^2+y^2)xydxdy \\
			&= \text{... (intermediate steps omitted for brevity) } \\
	\E(XY) 	&= \frac{3}{8} 
\end{align*}
Since $X$ and $Y$ have symmetric marginal distributions, $\E(X)=\E(Y)$. Thus,
\[
	\colvec{2}{\alpha}{\beta} = \frac{1}{\frac{7}{15} - \frac{25}{64}}\colvec{2}{\left(\frac{5}{8}\right)\left(\frac{7}{15}\right) - \left(\frac{5}{8}\right)\left(\frac{3}{8}\right)}{\left(\frac{3}{8}\right) - \left(\frac{5}{8}\right)\left(\frac{5}{8}\right)} = \frac{1}{73}\colvec{2}{55}{15}
\]

%%%________________________________________________________________%%%
\pagebreak
\section*{Question 8}
\begin{itemize}
	\item[4.1] \textbf{For $k\in\Z$, set $\mu_k=\E[Y^k]$}.
		\begin{enumerate}[(a)]
			\item \textbf{Construct an estimator, $\hat{\mu}_k$, for $\mu_k$} \\
				\[
					\hat{\mu}_k = \frac{1}{n}\sum_{i=1}^nY_i^k
				\]
			
			\item \textbf{Show that $\hat{\mu}_k$ is unbiased} \\
				As long as $\{Y_i\}_{i=1}^n$ are i.i.d., then,
				\[
					\E[\hat{\mu}_k] = \E[\frac{1}{n}\sum_{i=1}^nY_i^k] = \frac{1}{n}\sum_{i=1}^n\E[Y_i^k] = \frac{1}{n}\sum_{i=1}^n\mu_k = \mu_k
				\]
			
			\item \textbf{Calculate the variance of $\hat{\mu}_k$. What assumption is needed for it to be finite?} \\
				By the weak law of large numbers, if $Var\left(\hat{\mu}_k\right)<\infty$,
				\[
					\sqrt{n}\left(\hat{\mu}_k - \mu_k\right)\rightarrow_d\N\left(0,V\right)
				\]
				Where $V=Var(\mu_k)$. Since ${\sqrt{n}\left(\hat{\mu} - \mu\right)\rightarrow_d\N\left(0,\sigma^2\right)}$ where $\sigma^2$ is the variance of $Y$, by the delta method, with ${g(\theta)=\theta^k}$,
				\[
					V= g'(\mu)^2\sigma^2 = \mu^{2k-2}k^2\sigma^2
				\]
			
			\item \textbf{Propose an estimator for the variance of $\hat{\mu}_k$.} \\
				\[
					\widehat{Var(\hat{\mu}_k} = \frac{1}{n}\sum_{i=1}^n \left(Y_i - \hat{\mu}_k\right)^3
				\]
			
		\end{enumerate}
		
	\pagebreak
	\item[4.2] \textbf{Calculate $\E\left[\left(\bar{y}-\mu\right)^3\right]$. Under what condition is it zero?} \\
		\begin{align*}
			E\left[(\oly{n}-\mu)^3\right] &= E\left[\left(\est{n}(Y_i-\mu)\right)^3\right] \\
				&= \frac{1}{n^3}E\left[\left(\left(\sum_{i=1}^n(Y_i-\mu)\right)^2 + 2\sum_{i\neq j}^n(Y_i-\mu)(Y_j-\mu)\right)\left(\sum_{i=1}^n(Y_i-\mu)\right)\right] \\
				&= \frac{1}{n^3}E\left[\left(\sum_{i=1}^n(Y_i-\mu)\right)^3 \right] + E\left[\sum_{i\neq j}^n(Y_i-\mu)(Y_j-\mu)\right] \\
				&+ E\left[2\sum_{i\neq j}^n(Y_i-\mu)(Y_j-\mu)+ 3\sum_{i\neq j\neq k}^n(Y_i-\mu)(Y_j-\mu)(Y_k-\mu)\right] \\
				&= \frac{1}{n^3}E\left[\left(\sum_{i=1}^n(Y_i-\mu)\right)^3 \right] \\
				&= \frac{1}{n^2}E\left[(Y_i-\mu)^3 \right] 
		\end{align*}
		This value is zero if $\E(Y_i)=\E(Y)=\mu=0$.
		
	\item[4.3] \textbf{Explain the difference between $\bar{y}$ and $\mu$, and the difference between ${\frac{1}{n}\sum_{i=1}^nX_iX'_i}$ and $\E[X_iX'_i]$.} \\
		The difference between $\overline{Y}$ and $\mu$ is that $\overline{Y}$ is a random variable that equals $\mu$ \textit{in expectation}, whereas $\mu$ is a constant. Furthermore, $\overline{Y}$ is both the mean of a random sample and an esimator of $\mu$, whereas $\mu$ is the mean of the full distribution of $Y$.
		\smallskip \\
		Similarly, $\frac{1}{n}\sum_{i=1}^n X_iX'_i$ is a random variable and estimator of $\E[X_iX'_i]$, which is a constant.
		
	\item[4.4] \textbf{True or False: if $\hat{e}_i$ is the OLS residual from the linear regression of $Y_i$ on $X_i$, then ${\sum_{i=1}^n X_i^2\hat{e}_i=0}$} \\
		True. Since $\E[e_i|X_i]=0$, $\E[\hat{e_i}]=e_i$. Then,
		\[
			E\left[\sum_{i=1}^n X_i^2\hat{e_i}\right] = E\left[E\left[\sum_{i=1}^n X_i^2\hat{e_i}|X_i\right]\right] = \left[X_i^2E\left[\sum_{i=1}^n \hat{e_i}|X_i\right]\right]
			=0
		\]
		
	\pagebreak
	\item[4.5] \textbf{Prove (4.15) and (4.16)} \\
		\begin{align*}
			\E[\widehat{\beta}|X] 	&= \E\left[\left(\sum_{i=1}^n X_iX'_i\right)^{-1}\left(\sum_{i=1}^n X_iY_i\right)\bigm|X\right]	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}\E\left[\left(\sum_{i=1}^n X_iY_i\right)\bigm|X\right]	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}\sum_{i=1}^n \E\left[X_iY_i\bigm|X\right]	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}\sum_{i=1}^n X_i\E\left[Y_i\bigm|X\right]	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}\sum_{i=1}^n X_iX_i'\beta	\\
			\E[\widehat{\beta}|X] 	&= \beta							&\text{(4.15)}	\\
			Var[\widehat{\beta}|X] 	&= Var\left(\left(\sum_{i=1}^n X_iX'_i\right)^{-1}\left(\sum_{i=1}^n X_iY_i\right)\bigm|X\right)	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}Var\left(\sum_{i=1}^n X_iY_i\bigm|X\right)\left(\sum_{i=1}^n X_iX'_i\right)^{-1}	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}X'_iVar\left(\sum_{i=1}^n Y_i\bigm|X\right)X_i\left(\sum_{i=1}^n X_iX'_i\right)^{-1}	\\
									&= \left(\sum_{i=1}^n X_iX'_i\right)^{-1}(X'_i\sigma^2IX_i)\left(\sum_{i=1}^n X_iX'_i\right)^{-1}	\\
			Var[\widehat{\beta}|X] 	&= (X'X)^{-1}(X'\Omega X)(X'X)^{-1}	&\text{(4.16)}	\\
		\end{align*}
		
	\pagebreak
	\item[4.6] \textbf{Prove Theorem 4.5} \\
		Suppose that $Var(\varepsilon|X)=\Omega$ for some positive semi-definite $\Omega$. Then for ${Y = X'\beta + \varepsilon}$, the linear unbiased estimator of $\beta$ is:
		\[
			\tilde{\beta} = \left(X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}Y
		\]
		Then, the variance of $\tilde{\beta}$ is:
		\begin{align*}
			Var\left(\tilde{\beta}\bigm|X\right) 	&= Var\left(\left(X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}Y\bigm|X\right)									\\
													&= \left(X'\Omega^{-1}X\right)^{-1}Var\left(X'\Omega^{-1}Y\bigm|X\right)\left(X'\Omega^{-1}X\right)^{-1}	\\
													&= \left(X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}Var\left(Y\bigm|X\right)\Omega^{-1}X\left(X'\Omega^{-1}X\right)^{-1}	\\
													&= \left(X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}\Omega\Omega^{-1}X\left(X'\Omega^{-1}X\right)^{-1}	\\
													&= \left(X'\Omega^{-1}X\right)^{-1}X'\Omega^{-1}X\left(X'\Omega^{-1}X\right)^{-1}	\\
			Var\left(\tilde{\beta}\bigm|X\right) 	&= \left(X'\Omega^{-1}X\right)^{-1}
		\end{align*}
\end{itemize}



%%%________________________________________________________________%%%




\end{document}












