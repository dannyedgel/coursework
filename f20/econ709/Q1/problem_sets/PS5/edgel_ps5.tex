%%% Econ709: Econometrics
%%% Fall 2020
%%% Danny Edgel
%%%
% Due on Canvas Sunday, October 11th, 11:59pm Central Time
%%%

%%%
%							PREAMBLE
%%%

\documentclass{article}

%%% declare packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{bm}
\usepackage{bbm}
\usepackage{changepage}
\usepackage{centernot}
\usepackage{graphicx}
\usepackage{fancyhdr}
	\fancyhf{} % sets both header and footer to nothing
	\renewcommand{\headrulewidth}{0pt}
    \rfoot{Edgel, \thepage}
    \pagestyle{fancy}
	
%%% define shortcuts for set notation
\newcommand{\N}{\mathcal{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\union}{\bigcup}
\newcommand{\intersect}{\bigcap}
\newcommand{\lmt}{\underset{x\rightarrow\infty}{\text{lim }}}
\newcommand{\neglmt}{\underset{x\rightarrow-\infty}{\text{lim }}}
\newcommand{\zerolmt}{\underset{x\rightarrow 0}{\text{lim }}}
\newcommand{\usmax}{\underset{1\leq k \leq n}{\text{max }}}
\newcommand{\intinf}{\int_{-\infty}^{\infty}}
\newcommand{\olx}[1]{\overline{X}_{#1}}
\newcommand{\oly}[1]{\overline{Y}_{#1}}
\newcommand{\est}[1]{\frac{1}{#1}\sum_{i=1}^{#1}}
\newcommand{\loge}[1]{\text{log}\left(#1\right)}

%%% define column vector command (from Michael Nattinger)
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

\makeatletter
\let\amsmath@bigm\bigm

\renewcommand{\bigm}[1]{%
  \ifcsname fenced@\string#1\endcsname
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\expandafter\amsmath@bigm\csname fenced@\string#1\endcsname}%
  {\amsmath@bigm#1}%
}


%________________________________________________________________%

\begin{document}

\title{	Problem Set \#5 }
\author{ 	Danny Edgel 										\\ 
			Econ 709: Economic Statistics and Econometrics I	\\
			Fall 2020											\\
		}
\maketitle\thispagestyle{empty}

%%%________________________________________________________________%%%

\noindent\textit{Collaborated with Sarah Bass, Emily Case, Michael Nattinger, and Alex Von Hafften}
%%%________________________________________________________________%%%

\section*{Question 1}
\begin{itemize}
	\item[(a)] For any $\varepsilon>0$, let $N=1/\varepsilon$. Then, $\frac{1}{n}<\varepsilon$. Therefore,
		\[
			\forall\varepsilon>0,\exists N\in\mathbb{N}\text{ s.t }n\geq N\Rightarrow|a_n-0|<\varepsilon
		\]
		Thus, $a_n\rightarrow0$ as $n\rightarrow\infty$
	\item[(b)] We have already proven that $\frac{1}{n}\rightarrow\infty$. $\forall n$, $sin(\frac{\pi n}{2})\in[-1,1]$, so if you let $N=1/\varepsilon$, it will still be the case that, for any $\varepsilon>0$, ${n\geq N\Rightarrow |a_n-0|<\varepsilon}$
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 2}
Consider:
\[
	X_n = 	\begin{cases}
				-n, &\text{ with probability } 1/n		\\
				0,  &\text{ with probability } 1 + 2/n 	\\
				n,  &\text{ with probability } 1/n		\\
			\end{cases}
\]

\begin{itemize}
	\item[(a)] $X_n$ converges to zero in probability if, $\forall\varepsilon>0$:
		\[
			\underset{n\rightarrow 0}{\text{lim }}P(|X_n-0|<\varepsilon)=0
		\]
		From question 1 we know that $\frac{1}{n}\rightarrow 0$ as $n\rightarrow\infty$. This is true also of $\frac{2}{n}$, so as $n\rightarrow\infty$, ${1+\frac{2}{n}\rightarrow1}$. Thus, $P(X_n=0)\rightarrow1$ as $n\rightarrow\infty$. So $X_n$ converges to zero in probability.
		
	\item[(b)] 
		\begin{align*}
			E(X_n) &= P(X_n=-n)(-n) + P(X_n=0)(0) + P(X_n=n)(n)  \\
			E(X_n) &= \left(\frac{1}{n}\right)(-n) + 0 + \left(\frac{1}{n}\right)(n)=-1 + 1  = 0
		\end{align*}
	
	\item[(c)] 
		\begin{align*}
			Var(X_n) &= E(X_n^2) - \left[E(X_n)\right]^2 = P(X_n=-n)(-n)^2 + P(X_n=0)(0)^2 + P(X_n=n)(n)^2 - 0^2	\\
			Var(X_n) &= \left(\frac{1}{n}\right)n^2 + 0 + \left(\frac{1}{n}\right)n^2 = n + n = 2n
		\end{align*}
	
	\item[(d)] 
		\[
			E(X_n) = P(X_n=0)(0) + P(X_n=n)(n) = 0 + \left(\frac{1}{n}\right)(n) = 1
		\]
		
	\item[(e)] With the modified distribution from (d), $X_n$ will still converge in probability to 0. Clearly, $X_n\rightarrow_p 0$ as $n\rightarrow 0$ is not sufficient for $E(X_n)\rightarrow_p 0$. 
		
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 3}
Let $\overline{Y}^*=\est{n}w_iY_i$.

\begin{itemize}
	\item[(a)] 
		\[
			E(\overline{Y}^*) = E(\est{n}w_iY_i) = \est{n}w_iE(Y_i) = \mu\est{n}w_i = \mu
		\]
	\item[(b)] Letting $\sigma^2$ be the variance of $Y_i$,
		\[
			Var(\overline{Y}^*) = Var\left(\sum_{i=1}^nw_iY_i\right) = \frac{1}{n^2}\sum_{i=1}^nw_i^2Var(Y_i) = \left(\frac{1}{n^2}\sum_{i=1}^n w_i^2\right)\sigma^2
		\]
	
	\item[(c)] Let $\sum_{i=1}^n w_i^2$ be represented by the constant $k$. Then, by Chebyshev's inequality, $\forall\varepsilon>0$,
		\[
			P(|\overline{Y}^*-\mu|\geq\varepsilon)\leq \frac{Var(\overline{Y}^*)}{\varepsilon} = \frac{k\sigma^2}{\varepsilon n^2}
		\]
		where $\frac{k\sigma^2}{\varepsilon n^2}$ is equal to some constant times $\frac{1}{n^2}$, which converges to zero as $n\rightarrow\infty$. Thus, ${\overline{Y}^*\rightarrow_p\mu}$.
		
	\item[(d)] Another way of writing the probability from (c) is $\frac{\sigma^2}{\varepsilon}\sum_{i=1}^n\left(\frac{w_i}{n}\right)^2$. Thus, if we let ${w^*=\text{max}_{i\leq n}w_i}$,
		\[
			P(|\overline{Y}^*-\mu|\geq\varepsilon)\leq \frac{\sigma^2}{\varepsilon}\sum_{i=1}^n\left(\frac{w^*}{n}\right)^2
		\]
		So if $\left(\frac{w^*}{n}\right)^2\rightarrow 0$, then this probability also converges to 0.
	
	
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 4}
Each of the following answers assumes that the moment in question exists.
\begin{itemize}
	\item[(a)] $g(x)=x^2$ is continuous, so, under the continous mapping theorem, ${E(\est{n}X_i^2)\rightarrow_p E(X^2)}$ because ${E(X_i)\rightarrow_pE(X)}$
	
	\item[(b)] $g(x)=x^3$ is continuous, so, under the continous mapping theorem, ${E(\est{n}X_i^3)\rightarrow_p E(X^3)}$ because ${E(X_i)\rightarrow_pE(X)}$
	
	\item[(c)] $g(X_i)=max_{i\leq n}(X_i)$ is not a continuous function, so the continuous mapping theorem and weak law of large numbers cannot tell us anything about this statisic's convergence in probability
		
	\item[(d)] This statistic is the same as (a), but minus a constant. Thus, $g(\cdot)$ is continuous and the statistic converges, by the continuous mapping theorem
		
	\item[(e)] We know that $\sum_{i=1}^n X_i$ converges to $n\mu$ and that $\sum_{i=1}^nX_i$ converges to $nE(X^2)$. Further, the function applied to $X_i$, $g(x)=x$, where $x>0$, is continuous. So, by the continuous mapping theorem and weak law of large numbers, this statistic converges.
		
	\item[(f)] $\mathbbm{1}(\cdot)$ is not a continuous function and we don't know anything about the distribution of $X_i$, so the weak law of large numbers and continuous mapping theorem cannot tell us anything about the convergence of this statistic.
	
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 5}
Since $\{X_1,...,X_n\}$ is a random sample, we know that $\frac{1}{n}\sum_{i=1}^nX_i\rightarrow_pE[X]$. Then, by the continuous mapping theorem, for some continuous function $g(\cdot)$, $\frac{1}{n}\sum_{i=1}^ng(X_i)\rightarrow_pg(E[X])$. Now, let $g(x)=log(x)$. Then,
\[
	\loge{\hat{\mu}} = \loge{\left(\prod_{i=1}^n X_i\right)^{1/n}} = \frac{1}{n}\sum_{i=1}^n\loge{X_i}\rightarrow_pE(\loge{X})
\]
And by the contraction mapping theorem, ${g(X_i)\rightarrow_pg(X)\Rightarrow g^{-1}(X_i)\rightarrow g^{-1}(X)}$. So we can conclude:
\[
	\hat{\mu}\rightarrow_p e^{E(\loge{X})}=\mu
\]


%%%________________________________________________________________%%%

\section*{Question 6}
\begin{itemize}
	\item[(a)] $\hat{\mu}_k = \frac{1}{n}\sum_{i=1}^nX_i^k$ is a consistent estimator of $\mu_k=E(X^k)$, according to the weak law of large numbers.
	\item[(b)] Let $\sigma$ be the variance of $X^k$. Then, $g(x)=\sigma x$ is a continuous function, so ${X_i^k\rightarrow_dX^k\Rightarrow\sigma X_i^k\rightarrow_d\sigma X^k}$. By the central limit theorem, since $\{X_i\}$ is an i.i.d. sample, then if $\sigma^2_{X^k}<\infty$, ${\frac{\sqrt{n}(\hat{\mu}_k-\mu_k)}{\sigma_{X^k}}\rightarrow_d\N(0,1)}$. In the last assignment, we fount that ${Var(X_k)=\mu_{2k}-\mu_k}$. This value is clearly finite. Thus,
		\[
			\sigma_{X^k}\sqrt{n}((\hat{\mu}_k-\mu_k))\rightarrow_d\sigma_{X^k}\N(0,1)\sim\N(0,\mu_{2k}-\mu_k)
		\]
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 7}
\begin{itemize}
	\item[(a)] Since $g(x)=1/k$ is a continuous function and $\hat{\mu}_k$ converges to $\mu_k$, so $\hat{m}_k = \left(\frac{1}{n}\sum_{i=1}^nX_i^k\right)^{1/k}$ is a consistent estimator of $m_k=\left(E(X^k)\right)^{1/k}$ by the continuous mapping theorem.
	\item[(b)] Given the answer to 6(b), we know that ${\sigma_{X^k}\sqrt{n}((\hat{\mu}_k-\mu_k))\rightarrow_d\N(0,\mu_{2k}-\mu_k)}$, where $m_k=h(\mu_k)$. Then, since $h(x)=x^{1/k}$ is continuously differentiable, we can use the delta method to derive:
		\[
			\sqrt{n}\left(\hat{\mu}_k^{1/k}-\mu_k^{1/k}\right)\rightarrow_d\frac{1}{k}\mu_k^{\frac{k-1}{k}}\N(0,\mu_{2k}-\mu_k))\sim\N\left(0,\frac{1}{k^2}\mu_k^{\frac{2k-2}{k}}(\mu_{2k}-\mu_k)\right)
		\]
\end{itemize}


%%%________________________________________________________________%%%

\section*{Question 8}
\begin{itemize}
	\item[(a)] 
		\[
			\sqrt{n}(\hat{\beta}-\beta)\rightarrow_d2\mu\N(0,v^2)\sim\N(0,4(\mu v)^2)
		\]
	\item[(b)] If $\mu=0$, then the variance of $\hat{\beta}'s$ distribution is zero, meaning that $\hat{\beta}$'s distribution is just a mass at 0, equal to 1.
	\item[(c)] If $\mu=0$, then we know that $\sqrt{n}\hat{\mu}\rightarrow_d\N(0,v^2)$. Thus, we can use the continuous mapping theorem to calculate:
		\begin{align*}
			\frac{\sqrt{n}\hat{\mu}}{v}					&\rightarrow_d\N(0,1)			\\
			\left(\frac{\sqrt{n}\hat{\mu}}{v}\right)^2	&\rightarrow_d\N(0,1)			\\
			n\beta										&\rightarrow_d\N(0,v^4)
		\end{align*}
	\item[(d)] In part (a), $\hat{\beta}$ converged to its asymptotic distribution at a rate of $\sqrt{n}$, which had a variance that's proportionate to $\mu$. In (c), we find that, when $\mu=0$, $\hat{\beta}$ converges to its asymptotic distribution at a rate of $n$, and its variance is, intuitively, the square of $\hat{\mu}$'s variance. Unlike in (a), the variance does not depend on the value of $\mu$ itself.
\end{itemize}



%%%________________________________________________________________%%%




\end{document}












